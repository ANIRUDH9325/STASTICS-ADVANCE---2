{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCeHI4Q2v+223YbxM1Qqjj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANIRUDH9325/STASTICS-ADVANCE---2/blob/main/stastics_advance_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT0FWcxlhpli"
      },
      "outputs": [],
      "source": [
        "#1. Explain the properties of the F-distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is a continuous probability distribution that is often used in analysis of variance (ANOVA), regression analysis, and hypothesis testing to compare variances. Here are its key properties:\n",
        "\n",
        "1. Definition and Shape\n",
        "The F-distribution is defined as the ratio of two scaled chi-squared distributions. If\n",
        "ğ‘‹\n",
        "X and\n",
        "ğ‘Œ\n",
        "Y are independent chi-squared random variables with\n",
        "ğ‘‘\n",
        "1\n",
        "d\n",
        "1\n",
        "â€‹\n",
        "  and\n",
        "ğ‘‘\n",
        "2\n",
        "d\n",
        "2\n",
        "â€‹\n",
        "  degrees of freedom, respectively, then the F-distribution with\n",
        "ğ‘‘\n",
        "1\n",
        "d\n",
        "1\n",
        "â€‹\n",
        "  and\n",
        "ğ‘‘\n",
        "2\n",
        "d\n",
        "2\n",
        "â€‹\n",
        "  degrees of freedom is given by:\n",
        "ğ¹\n",
        "=\n",
        "(\n",
        "ğ‘‹\n",
        "/\n",
        "ğ‘‘\n",
        "1\n",
        ")\n",
        "(\n",
        "ğ‘Œ\n",
        "/\n",
        "ğ‘‘\n",
        "2\n",
        ")\n",
        "F=\n",
        "(Y/d\n",
        "2\n",
        "â€‹\n",
        " )\n",
        "(X/d\n",
        "1\n",
        "â€‹\n",
        " )\n",
        "â€‹\n",
        "\n",
        "The distribution is positively skewed, especially for small degrees of freedom, and becomes more symmetric as the degrees of freedom increase.\n",
        "2. Degrees of Freedom\n",
        "The F-distribution depends on two sets of degrees of freedom:\n",
        "ğ‘‘\n",
        "1\n",
        "d\n",
        "1\n",
        "â€‹\n",
        " : The degrees of freedom associated with the numerator (usually associated with the number of groups or treatments in ANOVA).\n",
        "ğ‘‘\n",
        "2\n",
        "d\n",
        "2\n",
        "â€‹\n",
        " : The degrees of freedom associated with the denominator (usually associated with the within-group variance).\n",
        "These two degrees of freedom determine the shape of the F-distribution.\n",
        "3. Range\n",
        "The F-distribution only takes positive values, as it is a ratio of squared quantities. Therefore,\n",
        "ğ¹\n",
        "â‰¥\n",
        "0\n",
        "Fâ‰¥0.\n",
        "As\n",
        "ğ¹\n",
        "â†’\n",
        "âˆ\n",
        "Fâ†’âˆ, the probability density tends toward zero.\n",
        "4. Mean and Variance\n",
        "The mean of an F-distribution exists if\n",
        "ğ‘‘\n",
        "2\n",
        ">\n",
        "2\n",
        "d\n",
        "2\n",
        "â€‹\n",
        " >2 and is given by:\n",
        "Mean\n",
        "=\n",
        "ğ‘‘\n",
        "2\n",
        "ğ‘‘\n",
        "2\n",
        "âˆ’\n",
        "2\n",
        "Mean=\n",
        "d\n",
        "2\n",
        "â€‹\n",
        " âˆ’2\n",
        "d\n",
        "2\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "The variance exists if\n",
        "ğ‘‘\n",
        "2\n",
        ">\n",
        "4\n",
        "d\n",
        "2\n",
        "â€‹\n",
        " >4 and is given by:\n",
        "Variance\n",
        "=\n",
        "2\n",
        "â‹…\n",
        "ğ‘‘\n",
        "2\n",
        "2\n",
        "â‹…\n",
        "(\n",
        "ğ‘‘\n",
        "1\n",
        "+\n",
        "ğ‘‘\n",
        "2\n",
        "âˆ’\n",
        "2\n",
        ")\n",
        "ğ‘‘\n",
        "1\n",
        "â‹…\n",
        "(\n",
        "ğ‘‘\n",
        "2\n",
        "âˆ’\n",
        "2\n",
        ")\n",
        "2\n",
        "â‹…\n",
        "(\n",
        "ğ‘‘\n",
        "2\n",
        "âˆ’\n",
        "4\n",
        ")\n",
        "Variance=\n",
        "d\n",
        "1\n",
        "â€‹\n",
        " â‹…(d\n",
        "2\n",
        "â€‹\n",
        " âˆ’2)\n",
        "2\n",
        " â‹…(d\n",
        "2\n",
        "â€‹\n",
        " âˆ’4)\n",
        "2â‹…d\n",
        "2\n",
        "2\n",
        "â€‹\n",
        " â‹…(d\n",
        "1\n",
        "â€‹\n",
        " +d\n",
        "2\n",
        "â€‹\n",
        " âˆ’2)\n",
        "â€‹\n",
        "\n",
        "5. Skewness\n",
        "The F-distribution is right-skewed, and this skewness is more pronounced with smaller degrees of freedom in the denominator. As the degrees of freedom increase, the distribution becomes more symmetric.\n",
        "6. Applications in Hypothesis Testing\n",
        "The F-distribution is widely used to test hypotheses concerning variances. For example, in ANOVA, it tests whether the variances between different groups are significantly different, by comparing the between-group variance to the within-group variance.\n"
      ],
      "metadata": {
        "id": "sZA6-nm3h9RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n"
      ],
      "metadata": {
        "id": "nVveBjzniEWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is commonly used in statistical tests that compare variances or evaluate multiple groups. Here are the main types of tests and why the F-distribution is appropriate for each:\n",
        "\n",
        "1. Analysis of Variance (ANOVA)\n",
        "Use: The F-distribution is central to ANOVA, where it tests for significant differences among group means.\n",
        "Why Appropriate: ANOVA divides total variability into \"between-group\" and \"within-group\" variances. The F-test assesses whether the variability between group means (explained variance) is significantly larger than the variability within groups (unexplained variance). This is done by comparing the ratio of between-group to within-group variances, which follows an F-distribution under the null hypothesis of equal group means.\n",
        "2. Regression Analysis (F-test for Overall Significance)\n",
        "Use: In regression analysis, the F-test checks if a regression model as a whole explains a significant portion of the variability in the dependent variable.\n",
        "Why Appropriate: This F-test compares the variance explained by the model (sum of squares due to regression) to the unexplained variance (sum of squares due to error). If the F-statistic is large, it suggests that the model provides a better fit to the data than a model with no predictors. The F-distribution models the ratio of explained to unexplained variance under the null hypothesis that the model has no explanatory power.\n",
        "3. Equality of Variances (Leveneâ€™s Test, Bartlettâ€™s Test)\n",
        "Use: Tests like Levene's test and Bartlettâ€™s test use the F-distribution to assess whether the variances across different groups are equal.\n",
        "Why Appropriate: These tests compare the sample variances across groups. The F-distribution is suited for this because it can model the ratio of two variances (numerator and denominator), making it ideal for testing homogeneity of variances.\n",
        "4. Comparing Two Variances (F-test for Variance Ratios)\n",
        "Use: An F-test can compare the variances of two independent samples to determine if they differ significantly.\n",
        "Why Appropriate: The F-distribution describes the ratio of two independent chi-squared distributed variables divided by their respective degrees of freedom. Since variances are based on squared deviations (and hence chi-squared distributions), the F-distribution is appropriate for testing whether one sample variance is significantly different from another.\n",
        "5. Generalized Linear Models (GLM)\n",
        "Use: The F-test is used in generalized linear models (GLMs) to assess the significance of individual predictors or groups of predictors.\n",
        "Why Appropriate: In GLMs, the F-distribution helps test whether the addition of certain predictors significantly improves the modelâ€™s explanatory power. The test statistic, based on a ratio of variances, follows an F-distribution under the null hypothesis that the predictors do not contribute significantly.\n",
        "Why the F-Distribution is Appropriate for These Tests\n",
        "Ratio of Variances: Since the F-distribution models the ratio of variances (or mean squared deviations), it is well-suited for tests that compare explained to unexplained variance.\n",
        "Skewness Reflects Sample Size: Its skewness, which decreases as sample sizes increase, aligns well with hypothesis testing principles, where larger samples provide more reliable, less skewed test statistics.\n",
        "Known Sampling Distribution Under Null Hypothesis: Under the null hypothesis of no effect, the distribution of the test statistic is known to follow an F-distribution, making it easy to determine p-values and critical values for hypothesis testing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT can make mistakes. Check important info."
      ],
      "metadata": {
        "id": "nQbnsM9yiLqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "#populations?"
      ],
      "metadata": {
        "id": "wYrUB9GJiSNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When conducting an F-test to compare the variances of two populations, several key assumptions must be met to ensure the validity of the test results. These assumptions are crucial because the F-test is sensitive to violations, which can affect the accuracy of the test. Here are the main assumptions:\n",
        "\n",
        "1. Normality of the Populations\n",
        "Both populations from which the samples are drawn should be normally distributed. The F-test is highly sensitive to deviations from normality, particularly if the sample sizes are small. Non-normal data can lead to incorrect conclusions about the equality of variances.\n",
        "2. Independence of Samples\n",
        "The samples from each population must be independent of each other. This means that the data from one sample should not influence or be related to the data from the other sample. Independence ensures that the variances are representative of each population without overlap or dependence.\n",
        "3. Random Sampling\n",
        "The samples must be randomly selected from each population. Random sampling ensures that the samples are unbiased and representative of the populations being compared, which is important for generalizability and validity.\n",
        "4. Ratio of Sample Variances Follows an F-distribution\n",
        "For the F-test to be valid, the ratio of the sample variances should follow an F-distribution under the null hypothesis that the population variances are equal. This condition is naturally satisfied if the assumptions above are met.\n",
        "5. Equality of Means (Optional)\n",
        "This is not a strict assumption for the F-test itself but can influence the interpretation. If the means of the two populations differ greatly, it may affect the variances, as variability could be influenced by the difference in central tendency. Some alternative tests (like Levene's test) are less sensitive to differences in means and may be preferred if this condition does not hold.\n",
        "Consequences of Violating Assumptions\n",
        "Normality: If the populations are not normal, the F-test may yield inaccurate results, especially with small sample sizes. In such cases, a nonparametric alternative (e.g., Levene's test) may be more appropriate.\n",
        "Independence: Lack of independence between samples can lead to incorrect estimates of variance and bias the F-test results.\n",
        "Random Sampling: Without random sampling, the test results may not generalize to the population, limiting the conclusions that can be drawn.\n",
        "When these assumptions are met, the F-test can reliably assess whether there is a significant difference between the variances of two populations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YqLra3B1ieJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. What is the purpose of ANOVA, and how does it differ from a t-test?\n"
      ],
      "metadata": {
        "id": "gOlMuCVwijKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of ANOVA (Analysis of Variance) is to test for significant differences between the means of three or more groups. It examines whether the variability between group means is greater than what would be expected by chance alone. Hereâ€™s a closer look at its purpose and how it differs from a t-test:\n",
        "\n",
        "Purpose of ANOVA\n",
        "Assess Differences Among Multiple Groups: ANOVA is used to determine if there are statistically significant differences between the means of three or more independent groups. It helps answer questions like, \"Are the average scores different across various levels of a treatment or factor?\"\n",
        "Analyze Variance Between and Within Groups: ANOVA partitions the total variability in the data into components due to between-group variability (differences among group means) and within-group variability (differences within each group). It then calculates an F-statistic, which tests whether the observed between-group variability is significant.\n",
        "Reduce Error from Multiple Comparisons: Unlike conducting multiple t-tests, ANOVA helps control the overall Type I error rate when comparing more than two groups, which reduces the risk of false positives.\n",
        "How ANOVA Differs from a t-test\n",
        "Number of Groups Compared:\n",
        "\n",
        "t-test: Typically compares the means of two groups (e.g., independent samples t-test for two groups or paired samples t-test for two related groups).\n",
        "ANOVA: Used when there are three or more groups to compare. A t-test is not efficient for multiple groups, as running multiple t-tests increases the likelihood of Type I errors.\n",
        "Type of Output (Statistic):\n",
        "\n",
        "t-test: Calculates a t-statistic based on the difference between two group means relative to the variability within the groups.\n",
        "ANOVA: Calculates an F-statistic based on the ratio of between-group variability to within-group variability. A large F-statistic suggests significant differences among group means.\n",
        "Error Control in Multiple Comparisons:\n",
        "\n",
        "t-test: Conducting multiple t-tests on the same dataset (for more than two groups) increases the chance of a Type I error, as each test has its own alpha level (typically 0.05).\n",
        "ANOVA: Controls for the risk of Type I errors across multiple groups by using a single F-test to assess differences among all group means simultaneously.\n",
        "Post-Hoc Tests for Specific Group Differences:\n",
        "\n",
        "t-test: Directly provides insight into which of the two groups differ.\n",
        "ANOVA: If ANOVA finds a significant effect (i.e., that at least one group differs), additional post-hoc tests (e.g., Tukey's, Bonferroni) are needed to identify which specific groups differ.\n",
        "When to Use ANOVA vs. a t-test\n",
        "Use a t-test if you are comparing only two groups.\n",
        "Use ANOVA if you are comparing three or more groups, as it is designed to handle multiple comparisons without inflating the Type I error rate.\n",
        "In summary, ANOVA is a more robust approach than the t-test for comparing multiple groups, allowing researchers to identify significant differences in means across multiple categories without increasing the risk of Type I error.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT can make mistakes. Check important info."
      ],
      "metadata": {
        "id": "sHQbVh49iuq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "#than two groups."
      ],
      "metadata": {
        "id": "WTKZCo0yivWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Controlling Type I Error Rate\n",
        "When comparing multiple groups (e.g., three or more), conducting separate t-tests for each pair of groups increases the risk of committing a Type I error (false positive). This is because each t-test carries its own probability of yielding a statistically significant result purely by chance.\n",
        "For example, if you have three groups, you would need to conduct three t-tests to compare each pair of means. For four groups, this would require six t-tests, and so on. As the number of groups grows, the total number of t-tests increases rapidly, raising the cumulative probability of a Type I error.\n",
        "One-way ANOVA avoids this issue by performing a single test to assess whether there is a statistically significant difference among all group means, controlling the overall Type I error rate.\n",
        "2. Efficiency and Simplicity\n",
        "Running a single ANOVA test is simpler and more efficient than performing multiple t-tests. A one-way ANOVA evaluates all group comparisons simultaneously within one analysis, which simplifies the interpretation and presentation of results.\n",
        "This also makes the process more straightforward, as ANOVA provides a single F-statistic and p-value for the overall test, indicating whether any group mean differs from the others.\n",
        "3. Interpreting Group Differences\n",
        "A one-way ANOVA tells us whether there is a statistically significant difference among group means but does not specify which groups differ. However, once ANOVA determines that there are differences, post-hoc tests (like Tukeyâ€™s, Bonferroni, or ScheffÃ© tests) can be used to determine which specific pairs of groups differ.\n",
        "This approach is more systematic and controlled than running multiple t-tests, allowing for targeted comparisons only after establishing an overall difference.\n",
        "4. Variance Analysis\n",
        "One-way ANOVA also provides insight into between-group and within-group variance. It partitions the total variability into variability due to differences between groups (explained variance) and variability within groups (unexplained variance). This breakdown is useful for understanding the structure of your data and the relative contributions of each source of variance.\n",
        "By analyzing variances, ANOVA is particularly suited for assessing differences when data points vary within groups, giving a better understanding of the factors contributing to the observed differences.\n",
        "Example Scenario\n",
        "Suppose a researcher wants to compare the effectiveness of four different teaching methods on students' test scores. Performing multiple t-tests would require six separate comparisons (e.g., Method A vs. B, A vs. C, A vs. D, etc.), increasing the chance of a Type I error. By using one-way ANOVA, the researcher can evaluate whether there is any significant difference in test scores among all four teaching methods in a single analysis, maintaining the error rate and simplifying the process.\n",
        "\n",
        "In summary, a one-way ANOVA is preferred over multiple t-tests when comparing more than two groups because it controls the Type I error rate, is more efficient, provides structured information on group differences, and allows for variance analysis. It ensures a rigorous and reliable way to test for differences among multiple groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT can make mistakes. Check important info."
      ],
      "metadata": {
        "id": "XBuzoXnziz-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "#How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mQL07WYii_OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "n ANOVA, variance is partitioned into two main components: between-group variance and within-group variance. This partitioning is fundamental to calculating the F-statistic, which helps determine if there are statistically significant differences among group means. Hereâ€™s how the partitioning works and contributes to the F-statistic calculation:\n",
        "\n",
        "1. Partitioning of Variance\n",
        "Total Variance (Total Sum of Squares, SST):\n",
        "\n",
        "The total variance measures the overall variation of all data points from the grand mean (the mean of all observations, regardless of group).\n",
        "Mathematically, it is the sum of the squared deviations of each data point from the grand mean.\n",
        "SST\n",
        "=\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘˜\n",
        "âˆ‘\n",
        "ğ‘—\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ‘–\n",
        "(\n",
        "ğ‘‹\n",
        "ğ‘–\n",
        "ğ‘—\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ".\n",
        ".\n",
        ")\n",
        "2\n",
        "SST=\n",
        "i=1\n",
        "âˆ‘\n",
        "k\n",
        "â€‹\n",
        "  \n",
        "j=1\n",
        "âˆ‘\n",
        "n\n",
        "i\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        " (X\n",
        "ij\n",
        "â€‹\n",
        " âˆ’\n",
        "X\n",
        "Ë‰\n",
        "  \n",
        "..\n",
        "â€‹\n",
        " )\n",
        "2\n",
        "\n",
        "where\n",
        "ğ‘‹\n",
        "ğ‘–\n",
        "ğ‘—\n",
        "X\n",
        "ij\n",
        "â€‹\n",
        "  is the\n",
        "ğ‘—\n",
        "j-th observation in the\n",
        "ğ‘–\n",
        "i-th group,\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ".\n",
        ".\n",
        "X\n",
        "Ë‰\n",
        "  \n",
        "..\n",
        "â€‹\n",
        "  is the grand mean,\n",
        "ğ‘˜\n",
        "k is the number of groups, and\n",
        "ğ‘›\n",
        "ğ‘–\n",
        "n\n",
        "i\n",
        "â€‹\n",
        "  is the number of observations in the\n",
        "ğ‘–\n",
        "i-th group.\n",
        "\n",
        "Between-Group Variance (Sum of Squares Between, SSB):\n",
        "\n",
        "The between-group variance measures how much the group means differ from the grand mean. It captures the variability due to differences between the groups.\n",
        "Mathematically, it is calculated as the sum of the squared deviations of each group mean from the grand mean, weighted by the number of observations in each group.\n",
        "SSB\n",
        "=\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘˜\n",
        "ğ‘›\n",
        "ğ‘–\n",
        "(\n",
        "ğ‘‹\n",
        "Ë‰\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ".\n",
        ".\n",
        ")\n",
        "2\n",
        "SSB=\n",
        "i=1\n",
        "âˆ‘\n",
        "k\n",
        "â€‹\n",
        " n\n",
        "i\n",
        "â€‹\n",
        " (\n",
        "X\n",
        "Ë‰\n",
        "  \n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "X\n",
        "Ë‰\n",
        "  \n",
        "..\n",
        "â€‹\n",
        " )\n",
        "2\n",
        "\n",
        "where\n",
        "ğ‘‹\n",
        "Ë‰\n",
        "ğ‘–\n",
        "X\n",
        "Ë‰\n",
        "  \n",
        "i\n",
        "â€‹\n",
        "  is the mean of the\n",
        "ğ‘–\n",
        "i-th group. This represents the \"explained\" variability, or the variability attributed to group differences.\n",
        "\n",
        "Within-Group Variance (Sum of Squares Within, SSW):\n",
        "\n",
        "The within-group variance measures the variation of individual data points around their respective group means. It reflects the \"unexplained\" variability, or the natural variation within each group.\n",
        "Mathematically, it is calculated as the sum of squared deviations of each data point from its respective group mean.\n",
        "SSW\n",
        "=\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘˜\n",
        "âˆ‘\n",
        "ğ‘—\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ‘–\n",
        "(\n",
        "ğ‘‹\n",
        "ğ‘–\n",
        "ğ‘—\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        "ğ‘–\n",
        ")\n",
        "2\n",
        "SSW=\n",
        "i=1\n",
        "âˆ‘\n",
        "k\n",
        "â€‹\n",
        "  \n",
        "j=1\n",
        "âˆ‘\n",
        "n\n",
        "i\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        " (X\n",
        "ij\n",
        "â€‹\n",
        " âˆ’\n",
        "X\n",
        "Ë‰\n",
        "  \n",
        "i\n",
        "â€‹\n",
        " )\n",
        "2\n",
        "\n",
        "This represents the variability within each group that cannot be attributed to the differences among groups.\n",
        "\n",
        "The total variance (SST) can then be expressed as the sum of between-group variance (SSB) and within-group variance (SSW):\n",
        "\n",
        "SST\n",
        "=\n",
        "SSB\n",
        "+\n",
        "SSW\n",
        "SST=SSB+SSW\n",
        "2. Calculation of Mean Squares\n",
        "Each sum of squares is then divided by its corresponding degrees of freedom to obtain the mean squares, which are used to calculate the F-statistic.\n",
        "Mean Square Between (MSB): The between-group sum of squares divided by the degrees of freedom for the groups,\n",
        "ğ‘‘\n",
        "ğ‘“\n",
        "ğµ\n",
        "=\n",
        "ğ‘˜\n",
        "âˆ’\n",
        "1\n",
        "df\n",
        "B\n",
        "â€‹\n",
        " =kâˆ’1.\n",
        "MSB\n",
        "=\n",
        "SSB\n",
        "ğ‘˜\n",
        "âˆ’\n",
        "1\n",
        "MSB=\n",
        "kâˆ’1\n",
        "SSB\n",
        "â€‹\n",
        "\n",
        "Mean Square Within (MSW): The within-group sum of squares divided by the degrees of freedom for error,\n",
        "ğ‘‘\n",
        "ğ‘“\n",
        "ğ‘Š\n",
        "=\n",
        "ğ‘\n",
        "âˆ’\n",
        "ğ‘˜\n",
        "df\n",
        "W\n",
        "â€‹\n",
        " =Nâˆ’k, where\n",
        "ğ‘\n",
        "N is the total number of observations.\n",
        "MSW\n",
        "=\n",
        "SSW\n",
        "ğ‘\n",
        "âˆ’\n",
        "ğ‘˜\n",
        "MSW=\n",
        "Nâˆ’k\n",
        "SSW\n",
        "â€‹\n",
        "\n",
        "3. Calculation of the F-Statistic\n",
        "The F-statistic is the ratio of the mean square between groups (MSB) to the mean square within groups (MSW):\n",
        "ğ¹\n",
        "=\n",
        "MSB\n",
        "MSW\n",
        "F=\n",
        "MSW\n",
        "MSB\n",
        "â€‹\n",
        "\n",
        "This ratio reflects how much larger the between-group variance is relative to the within-group variance. If the group means are similar, the between-group variance will be small relative to the within-group variance, leading to an F-statistic close to 1. However, if the group means are different, the between-group variance will be larger than the within-group variance, resulting in a larger F-statistic.\n",
        "4. Interpretation of the F-Statistic\n",
        "Under the null hypothesis (no difference between group means), the F-statistic is expected to follow an F-distribution with degrees of freedom\n",
        "(\n",
        "ğ‘˜\n",
        "âˆ’\n",
        "1\n",
        ",\n",
        "ğ‘\n",
        "âˆ’\n",
        "ğ‘˜\n",
        ")\n",
        "(kâˆ’1,Nâˆ’k).\n",
        "A larger F-statistic suggests a greater likelihood that the observed between-group variance is not due to random chance, implying that at least one group mean is significantly different from the others. The F-test uses this statistic to calculate a p-value, which helps decide whether to reject the null hypothesis.\n",
        "Summary\n",
        "By partitioning variance into between-group and within-group components, ANOVA isolates the variability due to group differences (explained variance) from the natural variability within groups (unexplained variance). The F-statistic, calculated as the ratio of these two variances, serves as the basis for testing whether the group means are significantly different.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT can make mistakes. Check important info."
      ],
      "metadata": {
        "id": "rs67hX8KjGws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "#differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n"
      ],
      "metadata": {
        "id": "paIXgKbujTdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classical (frequentist) and Bayesian approaches to ANOVA both aim to assess differences between group means, but they differ fundamentally in how they handle uncertainty, estimate parameters, and test hypotheses. Hereâ€™s a comparison of their key differences:\n",
        "\n",
        "1. Treatment of Uncertainty\n",
        "Frequentist Approach:\n",
        "\n",
        "The frequentist approach interprets probability in terms of long-run frequencies of outcomes. In classical ANOVA, uncertainty is managed through fixed values like confidence intervals and p-values, which are based on the sampling distribution of the test statistic under the null hypothesis.\n",
        "The frequentist perspective assumes that parameters (e.g., means and variances) are fixed but unknown quantities, and data variability reflects sampling variability.\n",
        "Bayesian Approach:\n",
        "\n",
        "In the Bayesian framework, probability represents a degree of belief or certainty in the presence of uncertainty. Bayesian ANOVA uses prior distributions to express beliefs about parameters before observing data and posterior distributions to update these beliefs after observing data.\n",
        "Parameters are considered random variables with probability distributions, capturing both prior knowledge and observed data in the analysis of uncertainty.\n",
        "2. Parameter Estimation\n",
        "Frequentist Approach:\n",
        "\n",
        "In frequentist ANOVA, parameters such as group means and variances are estimated by maximizing the likelihood of the observed data (e.g., through sample means and variances).\n",
        "Estimates are presented as single values (point estimates), and uncertainty around them is expressed in terms of confidence intervals. These intervals indicate a range within which the parameter would lie in a certain proportion of repeated samples (e.g., 95%).\n",
        "Bayesian Approach:\n",
        "\n",
        "Bayesian ANOVA provides posterior distributions for each parameter, representing a range of probable values rather than a single estimate. This includes the posterior means, variances, and credible intervals, which express the probability that a parameter lies within a certain range given the observed data.\n",
        "Instead of confidence intervals, Bayesian credible intervals (e.g., 95%) indicate the actual probability that the parameter lies within that range, given both the data and the prior.\n",
        "3. Hypothesis Testing\n",
        "Frequentist Approach:\n",
        "\n",
        "Classical ANOVA relies on a null hypothesis significance test (NHST) where the null hypothesis assumes that all group means are equal. The F-statistic is calculated as the ratio of between-group to within-group variance, and its p-value determines if the null hypothesis should be rejected.\n",
        "A significant p-value (typically < 0.05) indicates that the observed differences are unlikely to be due to chance, leading to a rejection of the null hypothesis. However, this approach does not quantify the probability of the null hypothesis being true.\n",
        "Bayesian Approach:\n",
        "\n",
        "Bayesian ANOVA allows for direct probability statements about hypotheses. Instead of relying on a p-value, Bayesian hypothesis testing evaluates models by calculating posterior probabilities for competing hypotheses.\n",
        "A Bayesian approach can also use Bayes factors to compare models (e.g., a model where group means differ vs. a model where they are equal). Bayes factors quantify the evidence in favor of one model over another and do not rely on a strict cutoff like 0.05.\n",
        "Bayesian ANOVA also enables testing of complex hypotheses and quantifies the probability of a hypothesis given the observed data.\n",
        "4. Role of Prior Information\n",
        "Frequentist Approach:\n",
        "\n",
        "In frequentist ANOVA, prior information does not influence the results, as all conclusions are based solely on the observed data.\n",
        "Frequentist methods assume that there is no prior knowledge about the parameters and treat each experiment independently of any previous knowledge.\n",
        "Bayesian Approach:\n",
        "\n",
        "Bayesian ANOVA explicitly incorporates prior knowledge or beliefs about parameters through prior distributions. For example, if prior studies indicate certain likely values for group means, this information can be encoded in prior distributions.\n",
        "The final inference reflects both the prior and the data, allowing for more flexibility in the presence of prior knowledge or in situations with limited data.\n",
        "5. Interpretation of Results\n",
        "Frequentist Approach:\n",
        "\n",
        "The frequentist approach interprets results in terms of hypothetical repeated sampling. For example, a 95% confidence interval implies that if we were to repeat the study many times, 95% of those intervals would contain the true parameter value.\n",
        "A p-value indicates the probability of obtaining results as extreme as or more extreme than the observed results, assuming the null hypothesis is true.\n",
        "Bayesian Approach:\n",
        "\n",
        "Bayesian results are interpreted directly in terms of probability. A 95% credible interval for a mean difference indicates a 95% probability that the true difference lies within that interval, given the observed data and prior beliefs.\n",
        "The Bayesian approach provides a direct probabilistic statement about the parameters and hypotheses, making it more intuitive for certain types of inference.\n",
        "Summary Table\n",
        "Feature\tFrequentist ANOVA\tBayesian ANOVA\n",
        "Uncertainty\tBased on sampling distributions\tBased on prior and posterior distributions\n",
        "Parameter Estimation\tPoint estimates and confidence intervals\tPosterior distributions and credible intervals\n",
        "Hypothesis Testing\tNull hypothesis, F-statistic, p-values\tPosterior probabilities, Bayes factors\n",
        "Prior Information\tNot incorporated\tIncorporated via prior distributions\n",
        "Interpretation\tIn terms of repeated sampling\tDirect probability statements about parameters\n",
        "When to Use Each Approach\n",
        "Frequentist ANOVA is generally preferred for standard hypothesis testing with large datasets and minimal prior information.\n",
        "Bayesian ANOVA is useful when incorporating prior knowledge, dealing with small samples, or when a direct probabilistic interpretation of hypotheses is desired. It can be particularly beneficial in research areas with established prior information or in situations where the frequentist assumptions (e.g., large sample sizes) are difficult to meet.\n",
        "Each approach has its strengths, and the choice often depends on the specific research context, available data, and whether prior knowledge is relevant."
      ],
      "metadata": {
        "id": "COmdoP5njcn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "#V Profession A: [48, 52, 55, 60, 62'\n",
        "#V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "#incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "#Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "#Objective: Gain experience in performing F-tests and interpreting the results in terms of variance compari\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for Profession A and Profession B\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate the variances of both professions (sample variances)\n",
        "var_A = np.var(profession_A, ddof=1)  # Sample variance for A\n",
        "var_B = np.var(profession_B, ddof=1)  # Sample variance for B\n",
        "\n",
        "# Perform the F-test: the F-statistic is the ratio of variances\n",
        "f_statistic = var_A / var_B  # F-statistic (var_A is the numerator)\n",
        "\n",
        "# Degrees of freedom for both samples\n",
        "df_A = len(profession_A) - 1  # Degrees of freedom for Profession A\n",
        "df_B = len(profession_B) - 1  # Degrees of freedom for Profession B\n",
        "\n",
        "# Calculate the p-value for the F-test\n",
        "p_value = 2 * min(stats.f.cdf(f_statistic, df_A, df_B), 1 - stats.f.cdf(f_statistic, df_A, df_B))\n",
        "\n",
        "# Output the F-statistic and p-value\n",
        "f_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CNkKVc_ojrAp",
        "outputId": "a6bd99aa-722b-4c7d-aecc-cd185722375a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.089171974522293, 0.49304859900533904)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBROB7mIkmyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the code:\n",
        "Data: The income data for Profession A and Profession B is provided as two lists.\n",
        "Variance Calculation: The variances of each profession's incomes are computed using np.var(), where ddof=1 ensures that we calculate the sample variance (not population variance).\n",
        "F-statistic: The F-statistic is calculated as the ratio of the variances of the two groups.\n",
        "Degrees of Freedom: The degrees of freedom for each group are calculated as n - 1, where n is the number of data points in each group.\n",
        "p-value: The p-value is calculated using the cumulative distribution function (stats.f.cdf()) for the F-distribution, considering both directions (since we are testing for equality of variances).\n",
        "This code will provide the F-statistic and p-value for the test. You can run this code in any Python environment with the numpy and scipy libraries installed."
      ],
      "metadata": {
        "id": "pZ2tWsSWkq8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "#average heights between three different regions with the following data1\n",
        "#V Region A: [160, 162, 165, 158, 164'\n",
        "#V Region B: [172, 175, 170, 168, 174'\n",
        "#V Region C: [180, 182, 179, 185, 183'\n",
        "#V Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "#V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for three regions\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform the one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "# Output the F-statistic and p-value\n",
        "f_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zRNagbu4kr1K",
        "outputId": "8d406894-13bd-4529-9501-2e4a2fe757e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Code:\n",
        "Data: The heights for Region A, Region B, and Region C are provided as lists.\n",
        "stats.f_oneway Function: This function from the scipy.stats module performs a one-way ANOVA test. It takes the data from the three regions as arguments and returns the F-statistic and the p-value.\n",
        "F-statistic: This value indicates the ratio of variance between groups to variance within groups. A higher F-statistic suggests that the group means differ more than expected by chance.\n",
        "p-value: This value indicates whether the observed differences between the group means are statistically significant. If the p-value is less than the significance level (typically 0.05), you can reject the null hypothesis, indicating that there is a significant difference in average heights between the regions.\n",
        "Interpretation of Results:\n",
        "If the p-value is less than 0.05, we reject the null hypothesis and conclude that there are significant differences in the average heights between the regions.\n",
        "If the p-value is greater than 0.05, we fail to reject the null hypothesis, suggesting that the average heights are not significantly different across the regions."
      ],
      "metadata": {
        "id": "1gkP-dN5lDEi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z61aY0pjk9GP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}